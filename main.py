import json
import requests
import torch
import os
import re
import gc
import scipy
import numpy as np
import ffmpeg
from datetime import datetime
from datetime import timezone
from diffusers import DiffusionPipeline
from diffusers.utils import load_image, export_to_video
from bark import SAMPLE_RATE, generate_audio
from transformers import AutoProcessor, BarkModel
from numba import cuda
import nltk
import librosa
import cv2
from prompts import prompts
from film_net import *

def _check_dir(func):
  def wrapper(*args, **kw):
      PATH = '../temp'
      if not os.path.exists(PATH):
          os.makedirs(PATH)
      output = func(*args, **kw)
      return output
  return wrapper

def _flush_vram(func):
  """
  This function clears VRAM but also makes CUDA unusable for future processes by 
  triggeringa runtime error.
  """
  def wrapper(*args, **kw):
      output = func(*args, **kw)
      device = cuda.get_current_device()
      device.reset()
      gc.collect()
      return output
  return wrapper

class StoryMaker:
    def __init__(self, prompt: str):
        self.prompt = prompt
        return

    def generate_scenes(self) -> None:
        """
        This function interacts with the AwanLLM API to generate a story based on the input prompt,
        then it calls parse_story to write outputs to scenes.json.

          Args:
            None
          Returns:
            None
        """
        secret = json.load(open('secret.json')) # load API key
        AWANLLM_API_KEY = secret['AWANLLM_API_KEY']

        message = prompts.LLAMA_PROMPT2.format(prompt=self.prompt)

        url = "https://api.awanllm.com/v1/completions"

        payload = json.dumps({
        "model": "Awanllm-Llama-3-8B-Dolfin",
        "prompt": message,
        "repetition_penalty": 1.1,
        "temperature": 0.8,
        "top_p": 0.9,
        "top_k": 40,
        "max_tokens": 1024,
        "stream": False
        })
        headers = {
        'Content-Type': 'application/json',
        'Authorization': f"Bearer {AWANLLM_API_KEY}"
        }

        n_retry = 0
        while n_retry < 5:
            try:
                response = requests.request("POST", url, headers=headers, data=payload)
                text = response.json()['choices'][0]['text']
                scenes = self.parse_story(text)

                # save generated content as scenes.json
                with open('scenes.json', 'w', encoding='utf-8') as f:
                  json.dump(scenes, f, ensure_ascii=False, indent=4)
                break

            except ValueError: # usually happens when LLM fails to follow the desired output format
                n_retry += 1
                print('retrying...')
                continue
        return None

    def parse_story(self, text):
        """
        This function parses the story generated by LLM. All text-to-image prompts are enclosed by <> and
        scripts are enclosed by []. The parsed strings are converted to lists and then stored in scenes dictionary.
        The function also checks if the length of the two lists are the same. If not, the function raises a ValueError.
        If the check is passed, the function creates a timestamp list in scenes, then returns the scenes dictionary.

          Arg:
            text (str): The output of LLM
          Returns:
            scenes (dict): A dictionary containing the text-to-image prompts, scripts, and timestamps.
        """
        scenes = {}
        text2image = re.findall('<(.*)>', text)
        script = re.findall('\[(.*)\]', text)
        print(f'Length of text2image: {len(text2image)}')
        print(f'Length of script: {len(script)}')

        if len(text2image) == len(script) and len(text2image) > 0:
            scenes['text2image'] = text2image
            scenes['script'] = script

            ts = datetime.now(timezone.utc).strftime('%Y-%m-%d-%H_%M_%S-{}') #file names for images and videos
            scenes['fname'] = [ts.format(i) for i in range(len(script))]
            print('Story generation complete!')
            print(scenes['script'])
            return scenes
        else:
            raise ValueError('The number of text2image and script do not match. Try generating the story again.')

    def paint(self, pipeline, prompt, id) -> None:
        """
        The function creates an image by using the pipeline and prompt provided.
        The image is then saved as a png file under the name provided.

          Args:
            pipeline: DiffusionPipeline
            prompt (str): The text prompt to use for generating the image.
            id (str): The file name of the image to save.
          Returns:
            None
        """
        STYLE_PROMPT = prompts.STYLE_PROMPT2
        NEGATIVE_PROMPT = prompts.NEGATIVE_PROMPT

        image = pipeline(
                prompt + STYLE_PROMPT,
                negative_prompt=NEGATIVE_PROMPT,
                width=1024,
                height=576,
                guidance_scale=12,
                target_size=(1024,576),
                original_size=(1980,1280),
                num_inference_steps=40
                ).images[0]

        image.save(f'../temp/{id}.png')
        print(f'Image saved as {id}.png')
        return None

    @_check_dir
    def generate_images(self) -> None:
        """
        This function loads a pretrained SDXL model and the scenes dict from scenes.json.
        It then generates images for each image prompt in scenes using the paint function.

          Args:
            None
          Returns:
            None
        """
        pipeline = DiffusionPipeline.from_pretrained("Lykon/dreamshaper-xl-1-0")
        pipeline.to("cuda")

        scenes = json.load(open('scenes.json'))
        for i in range(len(scenes['text2image'])):
            self.paint(pipeline, scenes['text2image'][i], scenes['fname'][i])

        pipeline = None
        return None

    def generate_speech(self) -> None:
      """
      generate_speech loads scenes dict from scenes.json and generates audio for each script in scenes with Suno Bark.
      The generated audio is saved as combined_audio_{id}.wav while the dutration of each scene is then stored in scenes.json.

        Args:
          None
        Returns:
          None
      """
      nltk.download('punkt')

      processor = AutoProcessor.from_pretrained("suno/bark")
      model = BarkModel.from_pretrained("suno/bark")
      SAMPLE_RATE = 22050
      HISTORY_PROMPT = "v2/en_speaker_7"

      scenes = json.load(open('scenes.json'))
      durations = []

      for i in range(len(scenes['script'])):
        long_string = scenes['script'][i]
        sentences = nltk.sent_tokenize(long_string)
        chunks = ['']
        token_counter = 0

        for sentence in sentences:
          current_tokens = len(nltk.Text(sentence))
          if token_counter + current_tokens <= 250:
            token_counter = token_counter + current_tokens
            chunks[-1] = chunks[-1] + " " + sentence
          else:
            chunks.append(sentence)
            token_counter = current_tokens

        # Generate audio for each prompt
        audio_arrays = []
        for prompt in chunks:
          audio_array = generate_audio(prompt, history_prompt=HISTORY_PROMPT, silent=True)
          audio_arrays.append(audio_array)

          # Combine the audio files
          combined_audio = np.concatenate(audio_arrays)

        # Write the combined audio to a file
        audio_filename = f"../temp/combined_audio_{i}.wav"
        scipy.io.wavfile.write(audio_filename, SAMPLE_RATE, combined_audio)
        print(f'Audio saved as {audio_filename}')

        durations.append(librosa.get_duration(filename=audio_filename))

      scenes['duration'] = durations
      with open('scenes.json', 'w', encoding='utf-8') as f:
        json.dump(scenes, f, ensure_ascii=False, indent=4)
      return None

    @_check_dir
    def generate_video(self, fps=3) -> None:
        """
        This function loads a pretrained SVD model and the scenes dict from scenes.json.
        It then generates a video for each image recorded in scenes dict. To ensure the video is at least
        as long as the audio file, last frame of each generated frame is used to generate the next SVD video.
        Lastly, all generated frames are concatenated and saved as generated_video_{id}.mp4.

          Args:
            fps (int): Frames per second of the generated video.
          Returns:
            None
        """
        video_pipeline = DiffusionPipeline.from_pretrained("stabilityai/stable-video-diffusion-img2vid-xt",
                                             torch_dtype=torch.float16,
                                             variant="fp16").to("cuda")

        # # compile unet to speed up vid generation
        # video_pipeline.unet = torch.compile(video_pipeline.unet, mode="reduce-overhead", fullgraph=True)

        scenes = json.load(open('scenes.json'))
        fnames = scenes['fname']
        img_list= [f'../temp/{fname}.png' for fname in fnames]
        print(img_list)

        for i in range(len(img_list)):
          image = load_image(f'{img_list[i]}')

          if 'duration' in scenes:
            scene_duration = scenes['duration'][i]
          else: scene_duration = 8 # default duration

          vid_length = 24/fps # video length in seconds
          repeats = int(scene_duration / vid_length)
          print(f'repeats: {repeats}')

          frames = video_pipeline(
                      image = image,
                      num_inference_steps=20,
                      motion_bucket_id=110,
                      noise_aug_strength=0.05,
                      decode_chunk_size=8).frames[0]

          for j in range(repeats-1): # generate enough frames for the scene duration
            temp = video_pipeline(
                      image = frames[-1], # use the last generated frame
                      num_inference_steps=20,
                      motion_bucket_id=110,
                      noise_aug_strength=0.05,
                      decode_chunk_size=8).frames[0]
            frames = frames + temp

          export_to_video(frames, f'generated_video_{i}.mp4', fps=fps)
          print(f'Video saved as generated_video_{i}.mp4')
        return None

    @_check_dir
    def interpolate_video(self, vid_name, fps, out_name):
        """
        Generate more intermediate frames for the generated videos. Creates a new video with the interpolated frames in 24 fps.

        Args:
          vid_name (str): video file name
          fps (int): the fps of the original generated video
        Returns:
          None
        """
        vidcap = cv2.VideoCapture(vid_name)
        success,image = vidcap.read()
        count = 0

        while success:
          cv2.imwrite("../temp/frame%d.jpg" % count, image)     # save frames as JPEG files
          success,image = vidcap.read()
          count += 1

        times_to_interpolate = int(24/fps)
        interpolator = Interpolator()
        interpolated_frames = []
        _UINT8_MAX_F = float(np.iinfo(np.uint8).max)

        for i in range(count):
            image1 = load_image_film(f"../temp/frame{i}.jpg")
            image2 = load_image_film(f"../temp/frame{i+1}.jpg")
            input_frames = [image1, image2]
            temp = list(
                interpolate_recursively(input_frames, times_to_interpolate,
                                                    interpolator))
            interpolated_frames.extend(temp)

        export_to_video(interpolated_frames, f'{out_name}.mp4', fps=24)
        print(f'Video saved as {out_name}.mp4')
        return None

    def compile_story(self, prior_fps=3, out_path='../output.mp4') -> None:
        """
        Putting everthing into one video

        Args:
          prior_fps (int): fps specified for SVD
        Returns:
          None
        """
        scenes = json.load(open('scenes.json'))

        vid_list = [f'../temp/out{i}.mp4' for i in range(len(scenes['script']))]
        # add audio to video files
        for i in range(len(scenes['script'])):
          self.interpolate_video(f'../temp/generated_video_{i}.mp4', fps=prior_fps, out_name=f'../temp/combined_video_{i}')
          video = ffmpeg.input(f'../temp/combined_video_{i}.mp4')
          audio = ffmpeg.input(f'/temp/combined_audio_{i}.wav')
          out = ffmpeg.output(video, audio, vid_list[i], vcodec='copy', acodec='aac', strict='experimental').run()
        
        open('concat.txt', 'w').writelines([('file %s\n' % input_path) for input_path in vid_list])
        ffmpeg.input('concat.txt', format='concat', safe=0).output('output.mp4', c='copy').run()
        os.remove('concat.txt')

        filelist = [ f for f in os.listdir('../temp/')]
        for f in filelist:
            os.remove(os.path.join('../temp/', f))

        return None